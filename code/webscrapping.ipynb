{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapingBee:\n",
    "    def __init__(self, url_site):\n",
    "        \n",
    "        # mamorina compte scrapingBee @gmail dia alaina iny key iny dia apetaka eto dia mandeha\n",
    "        \n",
    "        key = \"9G9UA49KMOZT46FW014R726HPDYT3BSQU7HXRWGB3ZFNH4OZDUIMEQPMKWLMSOL4J8Z6I836OXCFXD0E\"\n",
    "        self.SCRAPINGBEE_API_KEY = key\n",
    "        self.endpoint = \"https://app.scrapingbee.com/api/v1\"\n",
    "        self.params = {\n",
    "            'api_key': self.SCRAPINGBEE_API_KEY,\n",
    "            'url': url_site,\n",
    "            'render_js': 'false', \n",
    "        }\n",
    "        \n",
    "        self.url_site = url_site\n",
    "        \n",
    "        self.page = requests.get(self.url_site)\n",
    "        \n",
    "    def getSoup(self):\n",
    "        response = requests.get(self.endpoint, params=self.params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print('Error with your request: ' + str(response.status_code))\n",
    "        \n",
    "        else:\n",
    "            print('there is a response')\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return soup\n",
    "        \n",
    "    def getSoupTraditionnal(self):\n",
    "        soup = BeautifulSoup(self.page.content,'html.parser')\n",
    "        return soup\n",
    "        \n",
    "        \n",
    "class FileCsv:\n",
    "    \n",
    "    def __init__(self, _path):\n",
    "        self.path = _path\n",
    "        \n",
    "    def setHeader(self,header):\n",
    "        with open(self.path, 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "        \n",
    "    def addColumn(self, row):\n",
    "        with open(self.path, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    def read(self, _path):\n",
    "        with open(_path,'r') as f:\n",
    "            spamreader = csv.reader(f, delimiter=',')\n",
    "            for row in spamreader:\n",
    "                print(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scraping a page with all details\n",
    "    \n",
    "class ScrapingHouse:\n",
    "    \n",
    "    def __init__(self,url_house):\n",
    "        self.url = url_house\n",
    "        \n",
    "        sb = ScrapingBee(url_house)\n",
    "        self.soup = sb.getSoup()\n",
    "\n",
    "        self.features = {\n",
    "            'page' : 0,\n",
    "            'features' : [],\n",
    "            'values' : []\n",
    "        }\n",
    "        self.path = \"\"\n",
    "        \n",
    "        self.liensVille = []\n",
    "        \n",
    "    def getData(self):\n",
    "        \n",
    "        if self.soup:\n",
    "            features = self.soup.find_all('div',class_='p24_propertyOverviewKey')\n",
    "            datas = self.soup.find_all('div',class_='p24_info')\n",
    "\n",
    "            print(np.shape(datas))\n",
    "            print(np.shape(features))\n",
    "\n",
    "            for i in range(np.shape(datas)[0]):\n",
    "                self.features['values'].append(datas[i].text)\n",
    "                features = datas[i].parent.parent.div.text\n",
    "                self.features['features'].append(features)\n",
    "\n",
    "            # adding the adress if exist\n",
    "\n",
    "            a = self.soup.find('a',class_=\"p24_addressPropOverview\")\n",
    "\n",
    "            if a:\n",
    "                adress = a.text\n",
    "                features_adress = a.parent.parent.div.text\n",
    "                self.features['features'].append(features_adress)\n",
    "                self.features['values'].append(adress)\n",
    "\n",
    "        print(self.features)\n",
    "        \n",
    "        \n",
    "            \n",
    "#         for i in range(np.shape(features)[0]):\n",
    "# #             self.features['features'].append(features[i].text)\n",
    "#             print(features[i].text)\n",
    "    \n",
    "#         print('--------------')\n",
    "        \n",
    "#         for i in range(np.shape(datas)[0]):\n",
    "#             print(datas[i].text)\n",
    "            \n",
    "#         print(self.features)\n",
    "        \n",
    "#         for i in range(np.shape(datas)[0]):\n",
    "#             self.features['values'].append(datas[i].text)\n",
    "            \n",
    "        return self.features\n",
    "    \n",
    "    def saveData(self,page):\n",
    "        \n",
    "        file = FileCsv('data/house/' + page + '.csv')\n",
    "        file.setHeader(self.features['features'])\n",
    "        file.addColumn(self.features['values'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapingProvince:\n",
    "    \n",
    "    def __init__(self,url_prov):\n",
    "        self.url_page = []\n",
    "        self.nb_page = 0\n",
    "        self.page = requests.get(url_prov)\n",
    "        self.url_prov = url_prov\n",
    "    \n",
    "    def findUrlPage(self):\n",
    "        sb = ScrapingBee(self.url_prov)\n",
    "        soup = sb.getSoup()\n",
    "#         print(soup)\n",
    "        \n",
    "        ul_pagination = soup.find_all(\"ul\",class_=\"pagination\")\n",
    "        li = ul_pagination[0].find_all(\"li\")\n",
    "\n",
    "        last_page = int(li[np.shape(li)[0]-1].a.get('data-pagenumber'))\n",
    "        \n",
    "        for i in range(last_page):\n",
    "            split = self.url_prov.split('?')\n",
    "\n",
    "            split[0] += '/p' + str(i+1) + '?'\n",
    "            url = split[0] + split[1]\n",
    "            self.url_page.append(url)\n",
    "            \n",
    "        print(self.url_page)\n",
    "            \n",
    "        return self.url_page\n",
    "        \n",
    "    def saveUrl(self,province):\n",
    "        file = FileCsv('data/province/page/' + province + '.csv')\n",
    "        file.setHeader(self.features['features'])\n",
    "        file.addColumn(self.features['values'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapingPage:\n",
    "    \n",
    "    def __init__(self,url_page):\n",
    "        self.url_page = url_page\n",
    "        self.soup = ScrapingBee(url_page).getSoup()\n",
    "        self.url_house = []\n",
    "        \n",
    "    def getAllUrlHouse(self):\n",
    "        soup = self.soup\n",
    "        div_container = soup.find_all('div','p24_regularTile')\n",
    "        list_href = []\n",
    "        \n",
    "        for i in range(np.shape(div_container)[0]):\n",
    "             list_href.append('https://www.property24.com' + div_container[i].a.get('href'))\n",
    "        \n",
    "        self.url_house = list_href\n",
    "        \n",
    "        print(self.url_house)\n",
    "        \n",
    "        return self.url_house\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Site:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.url_ville = []\n",
    "        self.url_page_ville = []  # ato misy lien ana page bdb\n",
    "        self.list_url_house = []\n",
    "        self.url_page = []\n",
    "        \n",
    "    def findUrlVille(self):\n",
    "        soup = ScrapingBee(self.url).getSoupTraditionnal()\n",
    "        \n",
    "        div = soup.find_all('div',class_='p24_popular')\n",
    "        \n",
    "        file = FileCsv('data/ville/url_ville.csv')\n",
    "        file.setHeader(['url_ville'])\n",
    "        \n",
    "        for i in range(np.shape(div)[0]):\n",
    "            a = div[i].find_all('a')\n",
    "            for j in range(np.shape(a)[0]):\n",
    "                if j== np.shape(a)[0]-3:\n",
    "                    break\n",
    "                else:\n",
    "                    url = 'https://www.property24.com'+ a[j].get('href') + '?PropertyCategory=House%2cApartmentOrFlat'\n",
    "                    self.url_ville.append(url)\n",
    "                    file.addColumn([url])\n",
    "        \n",
    "    # prend en parametre un tableau d'url_ville et le parcours pour obtenir les liens_de son page\n",
    "    # liens des (1......5689)\n",
    "    \n",
    "    def findUrlPageVille(self, url_ville):\n",
    "        \n",
    "        url_page = []\n",
    "        \n",
    "        for i in range(np.shape(url_ville)[0]):\n",
    "            \n",
    "            sp = ScrapingProvince(url_ville[i])\n",
    "            url_page_one = sp.findUrlPage()\n",
    "            url_page.append(url_page_one)\n",
    "            self.url_page = url_page[0]\n",
    "        \n",
    "    def getContentPageVille(self,name_file):  # maka ny lien an'ilay house ao anaty ilay page\n",
    "        \n",
    "        file = FileCsv('data/house/house_' + name_file + '.csv')\n",
    "        file.setHeader(['data_house'])\n",
    "        \n",
    "        for i in range(np.shape(self.url_page)[0]):\n",
    "            sp = ScrapingPage(self.url_page[i])\n",
    "            \n",
    "            allUrlHouse = sp.getAllUrlHouse()\n",
    "            \n",
    "            for j in range(np.shape(allUrlHouse)[0]):\n",
    "                self.getDataHouse(allUrlHouse[j],file)\n",
    "                print('ok j')\n",
    "    \n",
    "    def getDataHouse(self,url_house,file):\n",
    "        \n",
    "        print(url_house)\n",
    "         \n",
    "        sh = ScrapingHouse(url_house)\n",
    "        features = sh.getData()\n",
    "        \n",
    "        file.addColumn(features['features'])\n",
    "        file.addColumn(features['values'])\n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site= Site('https://www.property24.com')\n",
    "print ('ok')\n",
    "# --- atao valeur an'io tableau io ilay lien ana province tiana ho alaina dia mahazo ny house rehetra ao aminy--\n",
    "# --- \n",
    "\n",
    "lien_province_tiana_alaina = 'https://www.property24.com/for-sale/eastern-cape/7?PropertyCategory=House%2cApartmentOrFlat'\n",
    "url_ville = [lien_province_tiana_alaina]\n",
    "\n",
    "site.findUrlPageVille(url_ville)\n",
    "\n",
    "# ovana ilay name_file dia ville no aloha dia province no farany, raha /mabopane/gauteng izany ilay lien ohatra dia lasa\n",
    "# mabopane_gauteng ilay fichier istoquer-na anazy\n",
    "\n",
    "name_file = \"eastern-cape_eastern-cape\"\n",
    "site.getContentPageVille(name_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
