{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperApi:\n",
    "    \n",
    "    def __init__(self,url):\n",
    "        _key = \"f510817b0c4b96aef3ebb365664c8dfc\"\n",
    "        self.key = _key\n",
    "        self.url_scraper =\"http://api.scraperapi.com?api_key=\" + self.key + \"&url=\" + url\n",
    "        print(self.url_scraper)\n",
    "        \n",
    "    def getSoup(self):\n",
    "        response = requests.get(self.url_scraper)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print('Error with your request: ' + str(response.status_code))\n",
    "        \n",
    "        else:\n",
    "            print('there is a response')\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapingBee:\n",
    "    def __init__(self, url_site):\n",
    "        \n",
    "        # mamorina compte scrapingBee @gmail dia alaina iny key iny dia apetaka eto dia mandeha\n",
    "        \n",
    "        key = \"24TOVPZDNT4YZC4NCYV73M27I1M00KHZOFFRDU223L1LA574CWAMJOUJRQAR5FTUW62AT5I6OCGGJB45\"\n",
    "        self.SCRAPINGBEE_API_KEY = key\n",
    "        self.endpoint = \"https://app.scrapingbee.com/api/v1\"\n",
    "        self.params = {\n",
    "            'api_key': self.SCRAPINGBEE_API_KEY,\n",
    "            'url': url_site,\n",
    "            'render_js': 'false', \n",
    "        }\n",
    "        \n",
    "        self.url_site = url_site\n",
    "        \n",
    "        self.page = requests.get(self.url_site)\n",
    "        \n",
    "    def getSoup(self):\n",
    "        response = requests.get(self.endpoint, params=self.params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print('Error with your request: ' + str(response.status_code))\n",
    "        \n",
    "        else:\n",
    "            print('there is a response')\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return soup\n",
    "        \n",
    "    def getSoupTraditionnal(self):\n",
    "        soup = BeautifulSoup(self.page.content,'html.parser')\n",
    "        return soup\n",
    "        \n",
    "        \n",
    "class FileCsv:\n",
    "    \n",
    "    def __init__(self, _path):\n",
    "        self.path = _path\n",
    "        \n",
    "    def setHeader(self,header):\n",
    "        with open(self.path, 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "        \n",
    "    def addColumn(self, row):\n",
    "        with open(self.path, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    def read(self, _path):\n",
    "        with open(_path,'r') as f:\n",
    "            spamreader = csv.reader(f, delimiter=',')\n",
    "            for row in spamreader:\n",
    "                print(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scraping a page with all details\n",
    "    \n",
    "class ScrapingHouse:\n",
    "    \n",
    "    def __init__(self,url_house):\n",
    "        self.url = url_house\n",
    "        \n",
    "        # sb = ScraperApi(url_house)\n",
    "        sb = ScrapingBee(url_house)\n",
    "        self.soup = sb.getSoup()\n",
    "\n",
    "        self.features = {\n",
    "            'page' : 0,\n",
    "            'features' : [],\n",
    "            'values' : []\n",
    "        }\n",
    "        self.path = \"\"\n",
    "        \n",
    "        self.liensVille = []\n",
    "        \n",
    "    def getData(self):\n",
    "        \n",
    "        if self.soup:\n",
    "            \n",
    "            features = self.soup.find_all('div',class_='p24_propertyOverviewKey')\n",
    "            datas = self.soup.find_all('div',class_='p24_info')\n",
    "            div_price = self.soup.find_all('div',class_=\"p24_mBM\")\n",
    "            \n",
    "            if features and datas and div_price:\n",
    "                \n",
    "                price = div_price[0].find_all('div','p24_price')[0].text \n",
    "                \n",
    "                for i in range(np.shape(datas)[0]):\n",
    "                    self.features['values'].append(datas[i].text)\n",
    "                    features = datas[i].parent.parent.div.text\n",
    "                    self.features['features'].append(features)\n",
    "\n",
    "                self.features['features'].append('price')\n",
    "                self.features['values'].append(price)\n",
    "\n",
    "\n",
    "                # adding the adress if exist\n",
    "\n",
    "                a = self.soup.find('a',class_=\"p24_addressPropOverview\")\n",
    "\n",
    "                if a:\n",
    "                    adress = a.text\n",
    "                    features_adress = a.parent.parent.div.text\n",
    "                    self.features['features'].append(features_adress)\n",
    "                    self.features['values'].append(adress)\n",
    "\n",
    "        print(self.features)\n",
    "        \n",
    "        \n",
    "            \n",
    "#         for i in range(np.shape(features)[0]):\n",
    "# #             self.features['features'].append(features[i].text)\n",
    "#             print(features[i].text)\n",
    "    \n",
    "#         print('--------------')\n",
    "        \n",
    "#         for i in range(np.shape(datas)[0]):\n",
    "#             print(datas[i].text)\n",
    "            \n",
    "#         print(self.features)\n",
    "        \n",
    "#         for i in range(np.shape(datas)[0]):\n",
    "#             self.features['values'].append(datas[i].text)\n",
    "            \n",
    "        return self.features\n",
    "    \n",
    "    def saveData(self,page):\n",
    "        \n",
    "        file = FileCsv('../data/house_price/' + page + '.csv')\n",
    "        file.setHeader(self.features['features'])\n",
    "        file.addColumn(self.features['values'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapingProvince:\n",
    "    \n",
    "    def __init__(self,url_prov):\n",
    "        self.url_page = []\n",
    "        self.nb_page = 0\n",
    "        self.page = requests.get(url_prov)\n",
    "        self.url_prov = url_prov\n",
    "    \n",
    "    def findUrlPage(self):\n",
    "        # sb = ScraperApi(self.url_prov)\n",
    "        sb = ScrapingBee(self.url_prov)\n",
    "        soup = sb.getSoup()\n",
    "#         print(soup)\n",
    "        \n",
    "        ul_pagination = soup.find_all(\"ul\",class_=\"pagination\")\n",
    "        li = ul_pagination[0].find_all(\"li\")\n",
    "\n",
    "        last_page = int(li[np.shape(li)[0]-1].a.get('data-pagenumber'))\n",
    "        \n",
    "        for i in range(last_page):\n",
    "            split = self.url_prov.split('?')\n",
    "\n",
    "            split[0] += '/p' + str(i+1) + '?'\n",
    "            url = split[0] + split[1]\n",
    "            self.url_page.append(url)\n",
    "            \n",
    "        print(self.url_page)\n",
    "            \n",
    "        return self.url_page\n",
    "        \n",
    "    def saveUrl(self,province):\n",
    "        file = FileCsv('../data/province/page/' + province + '.csv')\n",
    "        file.setHeader(self.features['features'])\n",
    "        file.addColumn(self.features['values'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapingPage:\n",
    "    \n",
    "    def __init__(self,url_page):\n",
    "        self.url_page = url_page\n",
    "        # self.soup = ScraperApi(url_page).getSoup()\n",
    "        self.soup = ScrapingBee(url_page).getSoup()\n",
    "        self.url_house = []\n",
    "        \n",
    "    def getAllUrlHouse(self):\n",
    "        soup = self.soup\n",
    "        div_container = soup.find_all('div','p24_regularTile')\n",
    "        list_href = []\n",
    "        \n",
    "        for i in range(np.shape(div_container)[0]):\n",
    "             list_href.append('https://www.property24.com' + div_container[i].a.get('href'))\n",
    "        \n",
    "        self.url_house = list_href\n",
    "        \n",
    "        print(self.url_house)\n",
    "        \n",
    "        return self.url_house\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Site:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.url_ville = []\n",
    "        self.url_page_ville = []  # ato misy lien ana page bdb\n",
    "        self.list_url_house = []\n",
    "        self.url_page = []\n",
    "        \n",
    "    def findUrlVille(self):\n",
    "        \n",
    "        # soup = ScraperApi(self.url).getSoup() \n",
    "        soup = ScrapingBee(self.url).getSoupTraditionnal()\n",
    "        \n",
    "        div = soup.find_all('div',class_='p24_popular')\n",
    "        \n",
    "        file = FileCsv('../data/ville/url_ville.csv')\n",
    "        file.setHeader(['url_ville'])\n",
    "        \n",
    "        for i in range(np.shape(div)[0]):\n",
    "            a = div[i].find_all('a')\n",
    "            for j in range(np.shape(a)[0]):\n",
    "                if j== np.shape(a)[0]-3:\n",
    "                    break\n",
    "                else:\n",
    "                    url = 'https://www.property24.com'+ a[j].get('href') + '?PropertyCategory=House%2cApartmentOrFlat'\n",
    "                    self.url_ville.append(url)\n",
    "                    file.addColumn([url])\n",
    "        \n",
    "    # prend en parametre un tableau d'url_ville et le parcours pour obtenir les liens_de son page\n",
    "    # liens des (1......5689)\n",
    "    \n",
    "    def findUrlPageVille(self, url_ville):\n",
    "        \n",
    "        url_page = []\n",
    "        \n",
    "        for i in range(np.shape(url_ville)[0]):\n",
    "            \n",
    "            sp = ScrapingProvince(url_ville[i])\n",
    "            url_page_one = sp.findUrlPage()\n",
    "            url_page.append(url_page_one)\n",
    "            self.url_page = url_page[0]\n",
    "        \n",
    "    def getContentPageVille(self,name_file):  # maka ny lien an'ilay house ao anaty ilay page\n",
    "        \n",
    "        file = FileCsv('../data/house_price/house_' + name_file + '.csv')\n",
    "        file.setHeader(['data_house'])\n",
    "        \n",
    "        for i in range(np.shape(self.url_page)[0]):\n",
    "            sp = ScrapingPage(self.url_page[i])\n",
    "            \n",
    "            allUrlHouse = sp.getAllUrlHouse()\n",
    "            \n",
    "            for j in range(np.shape(allUrlHouse)[0]):\n",
    "                self.getDataHouse(allUrlHouse[j],file)\n",
    "                print('ok j')\n",
    "    \n",
    "    def getDataHouse(self,url_house,file):\n",
    "        \n",
    "        print(url_house)\n",
    "        \n",
    "        sh = ScrapingHouse(url_house)\n",
    "        features = sh.getData()\n",
    "        \n",
    "        file.addColumn(features['features'])\n",
    "        file.addColumn(features['values'])\n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site= Site('https://www.property24.com')\n",
    "print ('ok')\n",
    "# --- atao valeur an'io tableau io ilay lien ana province tiana ho alaina dia mahazo ny house rehetra ao aminy--\n",
    "# --- \n",
    "\n",
    "lien_province_tiana_alaina = 'https://www.property24.com/for-sale/randburg/gauteng/8?PropertyCategory=House%2cApartmentOrFlat'\n",
    "url_ville = [lien_province_tiana_alaina]\n",
    "\n",
    "site.findUrlPageVille(url_ville)\n",
    "\n",
    "# ovana ilay name_file dia ville no aloha dia province no farany, raha /mabopane/gauteng izany ilay lien ohatra dia lasa\n",
    "# mabopane_gauteng ilay fichier istoquer-na anazy\n",
    "\n",
    "name_file = \"randburg_gauteng\"\n",
    "site.getContentPageVille(name_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class File:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.compt = 0\n",
    "        \n",
    "    def computeData(self):\n",
    "        compt = 0\n",
    "        with open(self.path,'r') as f:\n",
    "            spamreader = csv.reader(f, delimiter=',')\n",
    "            for row in spamreader:\n",
    "                if row != []:\n",
    "                    compt += 1\n",
    "                                \n",
    "        return compt//2\n",
    "    \n",
    "    def getFeatures(self):\n",
    "        features = []\n",
    "        self.compt = 0\n",
    "        \n",
    "        with open(self.path,'r') as f:\n",
    "            spamreader = csv.reader(f, delimiter=',')\n",
    "            \n",
    "            for row in spamreader:\n",
    "                \n",
    "                if row == []:\n",
    "                    a=1\n",
    "                else:\n",
    "                    self.compt += 1\n",
    "                    \n",
    "                    if self.compt == 2:\n",
    "                        features += row\n",
    "                    elif self.compt ==3 :\n",
    "                        self.compt = 1    \n",
    "                        \n",
    "        return list(OrderedDict.fromkeys(features))\n",
    "    \n",
    "\n",
    "    def getOneRow(self,features,values,features_final):\n",
    "\n",
    "        row = [[''] * np.shape(features_final)[0]] * 1\n",
    "        \n",
    "        n = np.shape(features)[0]\n",
    "        \n",
    "        # size features censé hitovy amin'ny size values\n",
    "        \n",
    "        for i in range(n):\n",
    "            index_features = features_final.index(features[i])\n",
    "            row[0][index_features] = values[i]\n",
    "            \n",
    "        return row\n",
    "            \n",
    "        \n",
    "    # method miretourne ny ligne @csv final, nan izay tsy misy anazy , omena features ilay itambarany dia iny no iaingana\n",
    "    \n",
    "    \n",
    "    def getRow(self,features_final):\n",
    "        \n",
    "        row_final = []\n",
    "        \n",
    "        with open(self.path,'r') as f:\n",
    "            spamreader = csv.reader(f, delimiter=',')\n",
    "            \n",
    "            features = []\n",
    "            values = []\n",
    "            \n",
    "            for row in spamreader:\n",
    "                \n",
    "                if row != []:\n",
    "                    self.compt += 1\n",
    "                    \n",
    "                    if self.compt == 2:\n",
    "                        features = row\n",
    "                        \n",
    "                    elif self.compt == 3 :\n",
    "                        self.compt = 1\n",
    "                        values = row\n",
    "                        \n",
    "                        row_final += self.getOneRow(features, values, features_final)\n",
    "                        \n",
    "        return row_final\n",
    "                        \n",
    "    \n",
    "class Data:\n",
    "    \n",
    "    def __init__(self, list_path):\n",
    "        self.list_path = list_path\n",
    "        \n",
    "    def computeData(self):\n",
    "        nb = 0\n",
    "        \n",
    "        for i in range(np.shape(self.list_path)[0]):\n",
    "            path = '../data/house/' + self.list_path[i]\n",
    "            f = File(path)\n",
    "            nb += f.computeData()\n",
    "        \n",
    "        return nb\n",
    "    \n",
    "    def getFeatures(self):\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for i in range(np.shape(self.list_path)[0]):\n",
    "            path = '../data/house/' + self.list_path[i]\n",
    "            f = File(path)\n",
    "            features += f.getFeatures()\n",
    "            features = list(OrderedDict.fromkeys(features))\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def save_one_csv(self):\n",
    "        \n",
    "        features_final = self.getFeatures()\n",
    "        row = []\n",
    "        file = FileCsv(\"../data/all_data.csv\");\n",
    "        file.setHeader(features_final)\n",
    "        \n",
    "        for i in range(np.shape(self.list_path)[0]):\n",
    "            path = '../data/house/' + self.list_path[i]\n",
    "            f = File(path)\n",
    "            row += f.getRow(features_final)\n",
    "            \n",
    "        for i in range(np.shape(row)[0]):\n",
    "            file.addColumn(row[i])\n",
    "            \n",
    "        print(\"All Data saved in the file : data/all_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemblage des fichiers en une seule et unique fichier 'all_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_file = listdir(\"../data/house\")\n",
    "\n",
    "data = Data(list_file)\n",
    "data.save_one_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
